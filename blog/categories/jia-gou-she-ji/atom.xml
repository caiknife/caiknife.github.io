<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: 架构设计 | CDEFGAB 1010110]]></title>
  <link href="http://caiknife.github.io/blog/categories/jia-gou-she-ji/atom.xml" rel="self"/>
  <link href="http://caiknife.github.io/"/>
  <updated>2019-02-20T14:02:23+08:00</updated>
  <id>http://caiknife.github.io/</id>
  <author>
    <name><![CDATA[CaiKnife]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[FEED流系统实现思想]]></title>
    <link href="http://caiknife.github.io/blog/2018/11/28/zhong-xiao-xing-fedliu-xi-tong-shi-xian/"/>
    <updated>2018-11-28T17:21:04+08:00</updated>
    <id>http://caiknife.github.io/blog/2018/11/28/zhong-xiao-xing-fedliu-xi-tong-shi-xian</id>
    <content type="html"><![CDATA[<p>从最早的Twitter、Facebook，到后来的微信朋友圈、信息流咨询，这些产品其实都是常见的FEED流系统。</p>

<p>那么今天思考一下，如何实现一个中小型FEED流系统。</p>

<!-- more -->


<p>在系统刚开始创建的时候，可以先考虑下面这两种方式：推模式和拉模式。</p>

<h3>推模式</h3>

<p>推模式，是发生在用户触发行为（发布新的动态，关注某个人，点赞）的时候。在触发时，用户的自身行为会记录到对应的行为表中，其次用户的行为也会记录到自己的粉丝对应动态表中。</p>

<p><a href="/downloads/image/feed/推模式.png" title="推模式 " class="fancybox"><img src="/downloads/image/feed/推模式.png" alt="推模式 " /></a></p>

<p>上面的流程大概是：</p>

<p><code>
用户A发布新的帖子（动态），帖子记录到帖子表（主表）中。
发帖行为塞到队列中。触发异步操作，消费者会先读取用户的粉丝列表（UID分表），依次写入到用户的动态表（UID分表）中。
前端读取用户动态FEED，使用过滤条件，读取用户的动态表（关联查询帖子表）。
</code></p>

<p>使用推方式，对需求变更是易适应的。</p>

<p>因为用户每一次的行为，我们都有存储相应的数据。即使变更，只需更改逻辑层代码。另外性能较好，后台数据已经准备好了，无需复杂的SQL查询。当然这样做，也存在很多弊端。</p>

<p><code>
如果在用户A发完动态后，其粉丝B取消关注了A。在这个时间差内，内容已经推送给粉丝B了。
数据量存储成本较大，假如一个用户的粉丝数是100万，在发帖后会写入100万条数据。
</code></p>

<h3>拉模式</h3>

<p>拉方式，是发生在粉丝拉取FEED时。粉丝拉取自己的动态，首先会检索自己的关注用户（UID分表）。得到关注的UID之后，再根据UID去查询关注用户发布的帖子。</p>

<p><a href="/downloads/image/feed/拉模式.png" title="拉模式 " class="fancybox"><img src="/downloads/image/feed/拉模式.png" alt="拉模式 " /></a></p>

<p>拉的模式相对是比较简单易实现的，另外对用户关系变更（新增，删除用户）是敏感的。其次也不存在数据存储压力。但在查询的时候，对帖子表本身压力是很大的。尤其是用户本身关注的人很多的话，会有很严重的性能问题。</p>

<p>下面可以使用另外一种方式来优化拉模式。</p>

<h3>拉方式优化-伪实时拉取</h3>

<p>用户在登录APP时，会发送用户活跃态到服务端。活跃信号塞到队列中，消费者依次读取活跃态UID，得到用户的关注者列表。得到关注者列表后，会去帖子表，查询关注人的发布的帖子。写到用户自己的FEED中。</p>

<p><a href="/downloads/image/feed/伪实时拉取.png" title="伪实时拉取 " class="fancybox"><img src="/downloads/image/feed/伪实时拉取.png" alt="伪实时拉取 " /></a></p>

<p>这种方式和对拉方式而言，能有效避免接口性能问题，相当于通过定时任务提前把用户的动态FEED跑出来。</p>

<p>和推方式比较，推是比较盲目的，这种方式只需针对活跃用户即可，能避免存储浪费。缺点在于实时性不好，用户登录APP后马上进入自己的FEED页，此时如果后台用户动态还没跑完，接口读取的就是历史数据了。当然这种方式不适合知乎，微博这种类型的APP的。</p>

<h3>拉方式优化-分区拉取</h3>

<p>分区拉取，是为了避免频繁查询单一帖子表所采用的一种优化手段。通过对帖子按照时间片分表，每次查询都能均摊到不同的表中，以此减轻主表的压力。</p>

<h3>推方式优化-定时推</h3>

<p>定时推，是以常驻进程的方式读取用户的发帖行为，再批量写入到粉丝的动态表中。这种方式和推方式差不多，只不过可以对多个发帖行为做聚合。</p>

<h3>推方式优化-特定用户推</h3>

<p>特定用户推，是推方式的一种优化方法。用户发送帖子时，只对活跃的粉丝用户写入。当然活跃用户的判定策略，是需要商定的。</p>

<h3>总结</h3>

<p>以上几种方案，都有自己的利弊和适用场景。选择最适合自己的就是最好的。</p>

<p>以上的方案还需要不断扩充完善。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[思考题：直播弹幕系统的设计]]></title>
    <link href="http://caiknife.github.io/blog/2018/11/25/zhi-bo-dan-mu-xi-tong-de-she-ji/"/>
    <updated>2018-11-25T20:45:36+08:00</updated>
    <id>http://caiknife.github.io/blog/2018/11/25/zhi-bo-dan-mu-xi-tong-de-she-ji</id>
    <content type="html"><![CDATA[<p>从来没有开发过弹幕系统，今天在被问到这个问题的时候有点不知所措，以为是传统的像 BiliBili 这样的弹幕视频网站一样，从存储系统中拉取已有弹幕数据交付给前端按时间顺序显示就好，但是事后重新思考这个的问题的时候，发现不对——绝对不是这么简单。当时我应该误解了对方的意思，没有及时作出沟通，一下子就懵了。</p>

<p>面试官想考察我的真正场景是——<strong><em>如何设计一个直播平台的弹幕系统</em></strong>。</p>

<p>晚上回来赶紧再复盘一下这个问题。</p>

<!-- more -->


<p>我依稀记得面试官的问题是这样的：</p>

<blockquote><p>不使用PUSH方式，不使用长连接的方案下，如何设计一个（直播平台的）弹幕系统，并且能够突出显示我自己发的弹幕。</p></blockquote>

<p>在B站这样的弹幕网站里，除开直播频道之外，每个单独的视频应该都是把已有的弹幕都存储起来，而且由于B站每个视频的弹幕是有上限的，这样就保证了数据不会超载，所以最简单的方式就是可以直接使用 redis 的 list 来实现，单条数据存储的可能是像下面这样的数据结构：</p>

<p>```js
{</p>

<pre><code>"member_id", // 用户ID
"content", // 弹幕内容
"offset_time", // 相对于视频时长的偏移时间，用于确定弹幕出现的位置
"timestamp", // 用户真正发表弹幕的时间
"extra" // 扩展字段，比如弹幕的效果（顶端，底端）、样式（颜色，字体大小）等等
</code></pre>

<p>}
```</p>

<p>当然如果稍做一些修改的话，也可以用 redis 的 sorted set 来实现。在这样的场景下，只需要在后端从存储中获取到每个视频对应的弹幕数据，排序好之后交给前端处理就好，甚至还可以不用后端做排序，让前端根据偏移时间自行做排序减少服务器的资源消耗。而要突出显示我自己的弹幕的话，只需要写完弹幕发送的时候，直接由前端处理先实时显示在屏幕上，然后再上报给后端接口存储起来就好。</p>

<p><strong><em>但是，直播系统的弹幕和这上面的思路完全不一样！！！</em></strong></p>

<p>直播间消息，相对于 IM 的场景，有其几个特点：1、消息要求及时，过时的消息对于用户来说不重要；2、松散的群聊，用户随时进群，随时退群；3、用户进群后，离线期间的消息不需要重发。</p>

<p>对于用户来说，在直播间有三个典型的操作：1、进入直播间，拉取正在观看直播的用户列表；2、接收直播间持续接收弹幕消息；3、自己发消息。</p>

<p>在这样的场景下，初步的设计可以做成这样——选择了 redis 的 sorted set 存储消息，基本操作如下:</p>

<blockquote><p>用户发弹幕，通过 zAdd 添加数据，其中 score 是弹幕的发送时间；</p>

<p>接收直播间的消息，通过 zRangeByScore 操作，两秒一次轮询；</p>

<p>进入直播间，获取用户的列表，通过 zRange 操作来完成；</p></blockquote>

<p>整个系统的流程应该是：</p>

<blockquote><p>写流程是:  前端提交弹幕给后端 &ndash;> 后端将弹幕推入队列 &ndash;> 队列处理机进行处理 &ndash;> 存储到 redis</p>

<p>读流程是:  前端轮询请求后端 &ndash;> 后端使用 zRangeByScore查询 redis &ndash;> 前端按时间顺序显示弹幕</p></blockquote>

<p>这个初步方案可能只能在直播人数较少的情况下起效，随着人数越来越多，瓶颈很快就能达到，会产生一些问题。</p>

<p>第一个问题——<strong><em>消息串行写入 redis，如果某个直播间消息量很大，那么消息会堆积在队列中，消息延迟较大。</em></strong></p>

<p>这个问题需要使用合适的消息队列来进行处理，由于我目前使用的最多的消息队列只有基于 redis 的 resque 和基于 Golang 的 nsque。没有做过详尽的性能测试来确定这两种队列能处理多大的 QPS，如果可以的话那就自然最好；如果不行的话，那就要选择更高性能的比如 Kafka 或者其他的分布式消息队列。</p>

<p>第二个问题——<strong><em>用户轮询最新消息，需要进行 redis 的 zRangeByScore 操作，redis slave 的性能瓶颈较大。</em></strong></p>

<p>解决这个问题可以额外增加一层缓存。后端每隔 1 秒左右取拉取一次直播间的弹幕，前端轮询数据时，从该缓存读取数据。弹幕的返回条数根据直播间的大小自动调整，小直播间返回允许时间跨度大一些的弹幕，大直播间则对时间跨度以及弹幕条数做更严格的限制。这里缓存与平常使用的本地缓存问题，有一个最大区别：成本问题。如果所有直播间的弹幕都进行缓存，假设同时有 1000 个直播间，每个直播间有5种弹幕类型，缓存每隔 1 秒拉取一次数据，40 台缓存处理机器，那么对 redis 的访问 QPS 是 1000 * 5 * 40 = 20W。成本太高，因此我们只有大直播间才自动开启缓存，小直播间不开启。</p>

<p>第三个问题——<strong><em>弹幕数据也支持回放，直播结束后，这些数据存放于 redis 中，在回放时，会与直播的数据竞争 redis 的 CPU 资源。</em></strong></p>

<p>解决方案——直播结束后，数据备份到 MySQL；增加一组回放的 redis；增加回放的缓存。回放时，读取数据顺序是: 缓存 &ndash;> redis &ndash;> MySQL。缓存与回放 redis 都可以只存某个直播间某种弹幕类型的部分数据，有效控制容量；缓存与回放 redis 使用 sorted set数据结构，这样整个系统的数据结构都保持一致。</p>

<p>我个人能力有限，暂时只能想到这么多。除了上面这些之外，还需要考虑整个系统的高可用保障——包括机房部署、降级和熔断、全面的业务监控、轮询方案的优化等等。</p>

<p>暂时就这些，我的知识面还需要不断完善，业务场景还需要不断扩充。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[思维导图：如何设计一个秒杀系统]]></title>
    <link href="http://caiknife.github.io/blog/2018/11/24/ru-he-she-ji-ge-miao-sha-xi-tong/"/>
    <updated>2018-11-24T12:13:21+08:00</updated>
    <id>http://caiknife.github.io/blog/2018/11/24/ru-he-she-ji-ge-miao-sha-xi-tong</id>
    <content type="html"><![CDATA[<p>秒杀是一个非常常见的应用场景了，我也做过一些秒杀类的需求。</p>

<p>做过这么多同类型的需求之后，我查阅了一些资料，总结了一下在设计一个秒杀系统中，可能会用到的一些知识点。</p>

<!-- more -->


<p><a href="/downloads/image/miaosha/miaosha.png" title="如何设计一个秒杀系统 " class="fancybox"><img src="/downloads/image/miaosha/miaosha.png" alt="如何设计一个秒杀系统 " /></a></p>

<p>这个思维导图我还会不断完善，不断迭代。</p>

<p>无规矩不成方圆，方法论就是规矩，动手实操才能成方圆。</p>

<p>这就是知识带来的力量。</p>
]]></content>
  </entry>
  
</feed>
